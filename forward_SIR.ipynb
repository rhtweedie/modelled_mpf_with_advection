{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from netCDF4 import Dataset, MFDataset\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import os.path\n",
    "import h5py as h5\n",
    "from scipy.spatial import KDTree\n",
    "import pyproj\n",
    "from scipy.interpolate import griddata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buoys:\n",
    "    \n",
    "    global rad, r_earth\n",
    "    rad=np.pi/180.0 # radiant <-> degree\n",
    "    r_earth=6.3675*10**6 # radius of Earth in [m]\n",
    "    \n",
    "    def __init__(self, lon_start, lat_start, earliest_date_of_buoy, start_advect_date):\n",
    "        print(lon_start)\n",
    "        self.oldlon = lon_start * rad\n",
    "        self.oldlat = lat_start * rad\n",
    "        self.lon = lon_start * rad\n",
    "        self.lat = lat_start * rad\n",
    "        self.initlon = lon_start * rad\n",
    "        self.initlat = lat_start * rad\n",
    "        self.old_u = np.zeros(lon_start.shape)\n",
    "        self.old_v = np.zeros(lon_start.shape)\n",
    "        self.date = datetime.strptime(earliest_date_of_buoy, \"%Y-%m-%d %H:%M:%S\")\n",
    "        self.startdates = start_advect_date\n",
    "        #self.delta_x = np.zeros(lon_start.shape)\n",
    "        #self.delta_y = np.zeros(lon_start.shape)\n",
    "        #self.u_ice = np.zeros(lon_start.shape)\n",
    "        #self.v_ice = np.zeros(lon_start.shape)\n",
    "        \n",
    "    def getdate(self):\n",
    "        return self.date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "    def trajectory(self, new_u, new_v, delta_t):\n",
    "        #print(\"Update buoy positions. Integrate for \" + str(delta_t/3600.) + \" hours.\")\n",
    "        \n",
    "        #save old position in case the drifter leaves the domain\n",
    "        self.oldlon = self.lon # radiant\n",
    "        self.oldlat = self.lat # radiant\n",
    "        \n",
    "        #displacement vectors\n",
    "        deltax1 = self.old_u * delta_t\n",
    "        deltay1 = self.old_v * delta_t\n",
    "        deltax2 = new_u * delta_t\n",
    "        deltay2 = new_v * delta_t\n",
    "        \n",
    "        #Heun method (2nd order)\n",
    "        self.lon = self.lon + (0.5*(deltax1 + deltax2) / (r_earth*np.cos(self.lat.values)) )\n",
    "        self.lat = self.lat + (0.5*(deltay1 + deltay2) /  r_earth )\n",
    "        \n",
    "        # keep degree in range 0..360 and -90..90\n",
    "        lon_deg = self.lon/rad % 360\n",
    "        lat_deg = np.clip(self.lat/rad, -90., 90.)\n",
    "        self.lon = lon_deg*rad\n",
    "        self.lat = lat_deg*rad\n",
    "        \n",
    "        #update velocity here (old value was needed for heun method)\n",
    "        self.old_u=new_u\n",
    "        self.old_v=new_v\n",
    "        \n",
    "        # set positions to NaN before the buoy is supposed to move\n",
    "        #idx=getindices_beforestart(self.getdate(), self.startdates)\n",
    "        #lon_deg[idx] = np.nan\n",
    "        #lat_deg[idx] = np.nan\n",
    "        #self.lon[idx] = self.initlon[idx]\n",
    "        #self.lat[idx] = self.initlat[idx]\n",
    "        #self.old_u[idx]=0.\n",
    "        #self.old_v[idx]=0.\n",
    "        \n",
    "        # update time stamp\n",
    "        self.date = self.date + timedelta(seconds=delta_t)\n",
    "\n",
    "        return lon_deg, lat_deg\n",
    "\n",
    "\n",
    "\n",
    "# a useful function we'll need\n",
    "def length_of_latitude_circle(lat=85.):\n",
    "    r_earth=6.3675*10**6 # radius of Earth in [m]\n",
    "    rad=np.pi/180.0 # radiant <-> degree  \n",
    "    return 2*np.pi*r_earth*np.cos(lat*rad) / 1000. # km\n",
    "\n",
    "\n",
    "# find the buoys that are not to be advected yet (current date < start date)\n",
    "def getindices_beforestart(currentdate, startdates):\n",
    "        \n",
    "    indices=np.zeros(np.shape(startdates),dtype='bool')\n",
    "    for i,val in enumerate(startdates):\n",
    "        # don't advect yet\n",
    "        if currentdate < startdates[i]:\n",
    "            indices[i]=True\n",
    "            \n",
    "    return indices\n",
    "\n",
    "\n",
    "# load OSISAF data for Northern Hemisphere at a certain date\n",
    "def loaddate_ofOSISAF(datestring, hemisphere='nh'):\n",
    "    \n",
    "    # convert datestring to datetime object\n",
    "    thedate = datetime.strptime(datestring, \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # let's construct the file name, \n",
    "    # e.g. drift-velocities/archive/ice/drift_lr/merged/2019/09/\n",
    "    # ice_drift_nh_polstere-625_multi-oi_201909011200-201909031200.nc\n",
    "    pathtofile = \"/home/htweedie/melt_ponds/data/drift-velocities/archive/ice/drift_lr/merged/\"\n",
    "    # middle part\n",
    "    middlefilename=\"ice_drift_\"+hemisphere+\"_polstere-625_multi-oi_\"\n",
    "    # e.g. 201907291200-201907311200 (48hr span)\n",
    "    enddate=thedate + timedelta(days=2)\n",
    "    # YYYY/MM/ (from end date)\n",
    "    YYYYMM=enddate.strftime(\"%Y\")+\"/\"+enddate.strftime(\"%m\")+\"/\"\n",
    "    endfilename= thedate.strftime(\"%Y%m%d%H%M\") + \"-\" + enddate.strftime(\"%Y%m%d%H%M\") + '.nc'\n",
    "    \n",
    "    # the OSISAF file to be loaded\n",
    "    filename=pathtofile + YYYYMM + middlefilename + endfilename\n",
    "    \n",
    "    # take previous files in case there is a data gap\n",
    "    sd=thedate\n",
    "    ed=enddate\n",
    "    while os.path.isfile(filename)!=True:\n",
    "        # try previous file\n",
    "        sd=sd - timedelta(days=1)\n",
    "        ed=ed - timedelta(days=1)\n",
    "        # YYYY/MM/ (from end date)\n",
    "        YYYYMM=ed.strftime(\"%Y\")+\"/\"+ed.strftime(\"%m\")+\"/\"\n",
    "        endfilename= sd.strftime(\"%Y%m%d%H%M\") + \"-\" + ed.strftime(\"%Y%m%d%H%M\") + '.nc'\n",
    "        filename=pathtofile + YYYYMM + middlefilename + endfilename\n",
    "        print('data gap: try previous file '+filename+' ...')\n",
    "    \n",
    "    #print(\"loading \"+filename+ \" ...\") # Python3 needs brackets here\n",
    "    \n",
    "    # load the file\n",
    "    fl = Dataset(filename)\n",
    "    #xc=fl.variables['xc']\n",
    "    #yc=fl.variables['yc']\n",
    "    #XC,YC=np.meshgrid(xc,yc)\n",
    "    \n",
    "    # lon lat on grid\n",
    "    lon_start=np.copy(fl.variables['lon'])\n",
    "    lat_start=np.copy(fl.variables['lat'])\n",
    "\n",
    "    # lon lat at the end of the displacement\n",
    "    lon_end=np.squeeze(fl.variables['lon1'][0,:,:])\n",
    "    lat_end=np.squeeze(fl.variables['lat1'][0,:,:])\n",
    "    \n",
    "    # close the file\n",
    "    fl.close()\n",
    "    \n",
    "    # compute Ufield from end points and start points (48hour change)\n",
    "    deltalon=lon_end-lon_start\n",
    "    deltalon[deltalon>100.]=deltalon[deltalon>100.]-360.   # jump at -180..180\n",
    "    deltalon[deltalon<-100.]=deltalon[deltalon<-100.]+360. # jump at -180..180\n",
    "    Ufield=deltalon/48. *length_of_latitude_circle(lat=lat_start[:,:])/360. / 3.6 # km/h -> m/s\n",
    "    \n",
    "    # compute Vfield as well\n",
    "    Vfield=(lat_end-lat_start)/48. *length_of_latitude_circle(lat=0.)/360. / 3.6 #km/h -> m/s\n",
    "    \n",
    "    return Ufield, Vfield, lon_start, lat_start\n",
    "\n",
    "\n",
    "# nearest-neighbor interpolation, finds U,V at the position of the buoys using a fast KDTree method\n",
    "def find_UV_atbuoy_pos(lon_start,lat_start, Ufield,Vfield, objects):\n",
    "    \n",
    "    # (lon,lat) tuples of the underlying grid\n",
    "    A = np.array([lon_start[:,:].flatten(), lat_start[:,:].flatten()]).T # -180..180 assumed in OSISAF\n",
    "    # change to -180..180 as assumed in OSISAF data; in the trajectory code its 0..360\n",
    "    lon_adjust = objects.lon/rad\n",
    "    lon_adjust[lon_adjust>180.] = lon_adjust[lon_adjust>180.]-360.\n",
    "    # zip buoy (lon & lat) arrays to (lon,lat) tuples\n",
    "    tuples = np.column_stack((lon_adjust, objects.lat/rad)) \n",
    "    # fast KDTree nearest neighbor method\n",
    "    idx = spatial.KDTree(A).query(tuples)[1]\n",
    "    \n",
    "    return Ufield[idx], Vfield[idx]\n",
    "\n",
    "\n",
    "def WGS84toEASE2N(lon, lat):\n",
    "    '''Converts WGS84 coordinates to EASE2N.\n",
    "\n",
    "    Params:\n",
    "        lon (array): the WGS84 longitude to convert\n",
    "        lat (array): the WGS84 latitude to convert\n",
    "\n",
    "    Returns:\n",
    "        (x, y): the corresponding EASE2N x and y coordinates\n",
    "    '''\n",
    "\n",
    "    proj_EASE2N = pyproj.Proj(\"+proj=laea +lon_0=0 +lat_0=90 +x_0=0 +y_0=0 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\")\n",
    "    proj_WGS84 = pyproj.Proj(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs \")\n",
    "    return pyproj.transform(proj_WGS84, proj_EASE2N, lon, lat)\n",
    "\n",
    "\n",
    "def EASE2NtoWGS84(x, y):\n",
    "\n",
    "    EASE2 = \"+proj=laea +lon_0=0 +lat_0=90 +x_0=0 +y_0=0 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\"\n",
    "    WGS84 = \"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\"\n",
    "    transformer = pyproj.Transformer.from_crs(EASE2, WGS84)\n",
    "    lon, lat = transformer.transform(x, y)\n",
    "    return lon, lat\n",
    "\n",
    "\n",
    "def load_MISR(MISR_path):\n",
    "    '''Loads MISR data and coordinates from specified file path.\n",
    "\n",
    "    Params:\n",
    "        MISR_path (str): the file path from which to retrieve data\n",
    "\n",
    "    Returns:\n",
    "        data (np.array): roughness data retrieved from the specified file\n",
    "        lon, lat\n",
    "        x, y\n",
    "    '''\n",
    "\n",
    "    file = h5.File(MISR_path, 'r')\n",
    "    \n",
    "    # extract coord data\n",
    "    lon = np.array(file['GeoLocation']['Longitude'])\n",
    "    lat = np.array(file['GeoLocation']['Latitude'])\n",
    "    x = np.array(file['GeoLocation']['x'])\n",
    "    y = np.array(file['GeoLocation']['y'])\n",
    "\n",
    "    # extract roughness data\n",
    "    data = np.array(file['Roughness']['Roughness_2D_svm'])    \n",
    "    \n",
    "    file.close()\n",
    "\n",
    "    return data, lon, lat, x, y\n",
    "\n",
    "\n",
    "def predict_mpf(SIR, R0, l, tau, hnet):\n",
    "    '''\n",
    "    Predicts meltpond fraction given an input SIR, based on the model by Landy et al, 2015.\n",
    "    '''\n",
    "    R = R0 * np.exp(-l * SIR) + tau\n",
    "    return (1 - np.exp(-R * hnet))\n",
    "\n",
    "\n",
    "def interpolate_to_MISR(x_in, y_in, data, x_out, y_out):\n",
    "    '''\n",
    "    Interpolates data of the shape x_in, y_in to the shape of x_out, y_out.\n",
    "    \n",
    "    Params:\n",
    "        data: the data to be interpolated\n",
    "        x_in, y_in: the shape of the data to be interpolated\n",
    "        \n",
    "    Returns:\n",
    "        x_out, y_out: the shape to which the data will be interpolated'''\n",
    "    return griddata((x_in.ravel(), y_in.ravel()), data.ravel( ), (x_out.ravel(), y_out.ravel()), 'nearest').reshape(8000,8000) \n",
    "\n",
    "\n",
    "def format_date(year, month, day):\n",
    "    return f\"{year}-{month}-{day} 12:00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Processing 2017 ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_290270/871356593.py:183: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  return pyproj.transform(proj_WGS84, proj_EASE2N, lon, lat)\n",
      "/tmp/ipykernel_290270/871356593.py:183: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  return pyproj.transform(proj_WGS84, proj_EASE2N, lon, lat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data could not be retrieved for advection start date, 2017-04-01 12:00:00: [Errno 2] No such file or directory: b'/home/htweedie/melt_ponds/data/OLCI/olci/2017/data/mpd1_20170401.nc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_290270/871356593.py:183: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  return pyproj.transform(proj_WGS84, proj_EASE2N, lon, lat)\n",
      "/opt/anaconda3/envs/cpom/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/anaconda3/envs/cpom/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         -45.000000\n",
      "1         -44.942638\n",
      "2         -44.885166\n",
      "3         -44.827576\n",
      "4         -44.769867\n",
      "             ...    \n",
      "999995    135.230530\n",
      "999996    135.172729\n",
      "999997    135.115036\n",
      "999998    135.057465\n",
      "999999    135.000000\n",
      "Name: 2017-04-01 12:00:00, Length: 1000000, dtype: float64\n",
      "This is year 2017, day #1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_290270/871356593.py:183: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  return pyproj.transform(proj_WGS84, proj_EASE2N, lon, lat)\n",
      "/tmp/ipykernel_290270/871356593.py:183: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  return pyproj.transform(proj_WGS84, proj_EASE2N, lon, lat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data could not be retrieved for 2017-04-02 12:00:00: [Errno 2] No such file or directory: b'/home/htweedie/melt_ponds/data/OLCI/olci/2017/data/mpd1_20170402.nc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cpom/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/anaconda3/envs/cpom/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is year 2017, day #2 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_290270/871356593.py:183: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  return pyproj.transform(proj_WGS84, proj_EASE2N, lon, lat)\n",
      "/tmp/ipykernel_290270/871356593.py:183: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  return pyproj.transform(proj_WGS84, proj_EASE2N, lon, lat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data could not be retrieved for 2017-04-03 12:00:00: [Errno 2] No such file or directory: b'/home/htweedie/melt_ponds/data/OLCI/olci/2017/data/mpd1_20170403.nc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cpom/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/anaconda3/envs/cpom/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is year 2017, day #3 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_290270/871356593.py:183: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  return pyproj.transform(proj_WGS84, proj_EASE2N, lon, lat)\n",
      "/tmp/ipykernel_290270/871356593.py:183: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  return pyproj.transform(proj_WGS84, proj_EASE2N, lon, lat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data could not be retrieved for 2017-04-04 12:00:00: [Errno 2] No such file or directory: b'/home/htweedie/melt_ponds/data/OLCI/olci/2017/data/mpd1_20170404.nc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cpom/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/anaconda3/envs/cpom/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes saved.\n"
     ]
    }
   ],
   "source": [
    "year = 2017\n",
    "print(f'------ Processing {year} ------')\n",
    "\n",
    "YEAR = year\n",
    "START_DATE = f'{YEAR}-04-01 12:00:00'\n",
    "SPACING = 8\n",
    "DAYS_TO_FORWARD = 3\n",
    "delta_t = 86400  # in seconds\n",
    "\n",
    "# retrieve MPF coordinates\n",
    "coord_fn = '/home/htweedie/melt_ponds/data/OLCI/olci/LongitudeLatitudeGrid-n12500-Arctic.h5'\n",
    "coords = h5.File(coord_fn, 'r')\n",
    "mpf_lon =  np.array(coords['Longitudes'])\n",
    "mpf_lat = np.array(coords['Latitudes'])\n",
    "x_mpf, y_mpf = WGS84toEASE2N(mpf_lon, mpf_lat)\n",
    "\n",
    "# retrieve MISR data and coordinates\n",
    "fn = f'/home/ssureen/MISR_data_monthly/April {YEAR} Roughness.h5'\n",
    "sigma, sigma_lon, sigma_lat, sigma_x, sigma_y = load_MISR(fn)\n",
    "\n",
    "# take an even subset of the data to reduce computational requirements\n",
    "all_lats = sigma_lat[::SPACING, ::SPACING].ravel()\n",
    "all_lons = sigma_lon[::SPACING, ::SPACING].ravel()\n",
    "all_sigma = sigma[::SPACING, ::SPACING].ravel()\n",
    "\n",
    "num_points = len(all_lons)\n",
    "earliest_date = START_DATE\n",
    "advect_start_date = START_DATE\n",
    "dates = [advect_start_date]\n",
    "\n",
    "# create dataframes in which lat, lon and mpf data will be stored\n",
    "lons = np.zeros((1, num_points))*np.nan\n",
    "lats = np.zeros((1, num_points))*np.nan\n",
    "mpfs = np.zeros((1, num_points))*np.nan\n",
    "x = np.zeros((1, num_points))*np.nan\n",
    "y = np.zeros((1, num_points))*np.nan\n",
    "lats_df = pd.DataFrame(data=lats, index=dates)\n",
    "lons_df = pd.DataFrame(data=lons, index=dates)\n",
    "mpfs_df = pd.DataFrame(data=mpfs, index=dates)\n",
    "x_df = pd.DataFrame(data=x, index=dates)\n",
    "y_df = pd.DataFrame(data=y, index=dates)\n",
    "\n",
    "lats_df.loc[advect_start_date] = all_lats # np.arange(90.,110.,1.)\n",
    "lons_df.loc[advect_start_date] = all_lons # np.arange(90.,110.,1.)\n",
    "\n",
    "start_x, start_y = WGS84toEASE2N(all_lons, all_lats)\n",
    "x_df.loc[advect_start_date] = start_x\n",
    "y_df.loc[advect_start_date] = start_y\n",
    "\n",
    "# format the starting date, then load MPF data for that day\n",
    "date = datetime.strptime(advect_start_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "start_YYYYMMDD = date.strftime(\"%Y\")+date.strftime(\"%m\")+date.strftime(\"%d\")\n",
    "try:\n",
    "    if YEAR >= 2017 and YEAR <= 2023:\n",
    "        fn = f'/home/htweedie/melt_ponds/data/OLCI/olci/{YEAR}/data/mpd1_{start_YYYYMMDD}.nc'\n",
    "    elif YEAR >= 2002 and YEAR <= 2011:\n",
    "        fn = f'/home/htweedie/melt_ponds/data/MERIS/mecosi/{YEAR}/data/mpd1_{start_YYYYMMDD}.nc'\n",
    "    ds = xr.open_dataset(fn)\n",
    "    mpf = ds['mpf']\n",
    "except Exception as e:\n",
    "    mpf = np.zeros(num_points)\n",
    "    mpf[:] = np.nan\n",
    "    print(f'Data could not be retrieved for advection start date, {date}: {e}')\n",
    "\n",
    "# find all MPFs within each sigma grid cell\n",
    "tree = KDTree(list(zip(x_mpf.ravel(), y_mpf.ravel())))\n",
    "x_sigma, y_sigma = WGS84toEASE2N(all_lons, all_lats)\n",
    "max_radius = 10000\n",
    "indices_within_grid = tree.query_ball_point(list(zip(x_sigma, y_sigma)), r = max_radius)\n",
    "\n",
    "# calculate the mean MPF within the radius for each sigma grid cell, and add to df\n",
    "if len(indices_within_grid) > 0:\n",
    "    mean_mpf = np.zeros(num_points)\n",
    "    for i in range(num_points):\n",
    "        mean_mpf[i] = np.mean(np.asarray(mpf).ravel()[indices_within_grid[i]])\n",
    "mean_mpf[mean_mpf==0] = np.nan\n",
    "mpfs_df.loc[advect_start_date] = np.asarray(mean_mpf).ravel()\n",
    "\n",
    "# initialise 'Bouys' for all points to be advected\n",
    "points = Buoys(lons_df.loc[advect_start_date], lats_df.loc[advect_start_date], advect_start_date, earliest_date)\n",
    "\n",
    "# advect all points by the set number of days\n",
    "forwarded_mpfs = []\n",
    "for i in np.arange(1, DAYS_TO_FORWARD+1):\n",
    "    print(f'This is year {YEAR}, day #{i} of {DAYS_TO_FORWARD}')\n",
    "\n",
    "    Ufield, Vfield, lon_start, lat_start = loaddate_ofOSISAF(points.getdate(), hemisphere='nh')\n",
    "    U,V = find_UV_atbuoy_pos(lon_start, lat_start, Ufield.flatten(),Vfield.flatten(), points)\n",
    "\n",
    "    # don't advect buoys when there is no ice\n",
    "    fixed=np.logical_or(U.mask, V.mask)\n",
    "    U[fixed]=0.\n",
    "    V[fixed]=0.\n",
    "\n",
    "    LON,LAT = points.trajectory(U, V, delta_t=delta_t) # U,V in m/s, delta_t in seconds\n",
    "\n",
    "    # create dataframe with new lats and lons\n",
    "    new_lons = pd.DataFrame(LON.rename(points.getdate())).T\n",
    "    new_lats = pd.DataFrame(LAT.rename(points.getdate())).T\n",
    "    new_x, new_y = WGS84toEASE2N(LON, LAT)\n",
    "    date = [points.getdate()]\n",
    "    new_x_df = pd.DataFrame(new_x, columns=date).T\n",
    "    new_y_df = pd.DataFrame(new_y, columns=date).T   \n",
    "\n",
    "    # add dataframe with new lats and lons to original one\n",
    "    lons_df = pd.concat([lons_df, new_lons])\n",
    "    lats_df = pd.concat([lats_df, new_lats])\n",
    "    x_df = pd.concat([x_df, new_x_df])\n",
    "    y_df = pd.concat([y_df, new_y_df])\n",
    "    \n",
    "    x_sigma, y_sigma = WGS84toEASE2N(new_lons, new_lats)\n",
    "\n",
    "    # get and format current datestring\n",
    "    date = datetime.strptime(points.getdate(), \"%Y-%m-%d %H:%M:%S\")\n",
    "    YYYYMMDD = date.strftime(\"%Y\")+date.strftime(\"%m\")+date.strftime(\"%d\")\n",
    "\n",
    "    # retrieve MPF data for this day\n",
    "    try:\n",
    "        if YEAR >= 2017 and YEAR <= 2023:\n",
    "            fn = f'/home/htweedie/melt_ponds/data/OLCI/olci/{YEAR}/data/mpd1_{YYYYMMDD}.nc'\n",
    "        elif YEAR >= 2002 and YEAR <= 2011:\n",
    "            fn = f'/home/htweedie/melt_ponds/data/MERIS/mecosi/{YEAR}/data/mpd1_{YYYYMMDD}.nc'\n",
    "        ds = xr.open_dataset(fn)\n",
    "        mpf = ds['mpf']\n",
    "        print(f'Data retrieved for {date}')\n",
    "    except Exception as e:\n",
    "        mpf = np.zeros(896*608)\n",
    "        mpf[:] = np.nan\n",
    "        print(f'Data could not be retrieved for {date}: {e}')\n",
    "\n",
    "    # Query the tree to find all points within final_lons and final_lats grids\n",
    "    max_radius = 10000\n",
    "    indices_within_grid = tree.query_ball_point(list(zip(x_sigma.ravel(), y_sigma.ravel())), r = max_radius)\n",
    "\n",
    "    # calculate the mean MPF within the radius for each sigma grid point\n",
    "    if len(indices_within_grid) > 0:\n",
    "        mean_mpf = np.zeros(num_points)\n",
    "        for i in range(num_points):\n",
    "            mean_mpf[i] = np.mean(np.asarray(mpf).ravel()[indices_within_grid[i]])\n",
    "\n",
    "    # convert 0s to nans and append to list\n",
    "    mean_mpf[mean_mpf==0] = np.nan\n",
    "    forwarded_mpfs.append(mean_mpf)\n",
    "\n",
    "    ind = format_date(date.strftime(\"%Y\"), date.strftime(\"%m\"), date.strftime(\"%d\"))\n",
    "    new_mpfs = pd.DataFrame(data=np.asarray(mean_mpf).reshape(1, len(np.asarray(mean_mpf).ravel())), index=[ind])\n",
    "    mpfs_df = pd.concat([mpfs_df, new_mpfs])\n",
    "\n",
    "# save dataframe\n",
    "#mpfs_df.to_pickle(f'/home/htweedie/melt_ponds/data/forwarded_mpfs/mpf_from_{start_YYYYMMDD}_{DAYS_TO_FORWARD}_days_spacing_{SPACING}.pkl')\n",
    "#lons_df.to_pickle(f'/home/htweedie/melt_ponds/data/forwarded_mpfs/lon_from_{start_YYYYMMDD}_{DAYS_TO_FORWARD}_days_spacing_{SPACING}.pkl')\n",
    "#lats_df.to_pickle(f'/home/htweedie/melt_ponds/data/forwarded_mpfs/lat_from_{start_YYYYMMDD}_{DAYS_TO_FORWARD}_days_spacing_{SPACING}.pkl')\n",
    "print(f'Dataframes saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>999990</th>\n",
       "      <th>999991</th>\n",
       "      <th>999992</th>\n",
       "      <th>999993</th>\n",
       "      <th>999994</th>\n",
       "      <th>999995</th>\n",
       "      <th>999996</th>\n",
       "      <th>999997</th>\n",
       "      <th>999998</th>\n",
       "      <th>999999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-04-01 12:00:00</th>\n",
       "      <td>-3.999500e+06</td>\n",
       "      <td>-3.991500e+06</td>\n",
       "      <td>-3.983500e+06</td>\n",
       "      <td>-3.975500e+06</td>\n",
       "      <td>-3.967500e+06</td>\n",
       "      <td>-3.959500e+06</td>\n",
       "      <td>-3.951500e+06</td>\n",
       "      <td>-3.943500e+06</td>\n",
       "      <td>-3.935500e+06</td>\n",
       "      <td>-3.927500e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.920500e+06</td>\n",
       "      <td>3.928500e+06</td>\n",
       "      <td>3.936500e+06</td>\n",
       "      <td>3.944500e+06</td>\n",
       "      <td>3.952500e+06</td>\n",
       "      <td>3.960501e+06</td>\n",
       "      <td>3.968500e+06</td>\n",
       "      <td>3.976500e+06</td>\n",
       "      <td>3.984500e+06</td>\n",
       "      <td>3.992500e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-02 12:00:00</th>\n",
       "      <td>-3.999500e+06</td>\n",
       "      <td>-3.991500e+06</td>\n",
       "      <td>-3.983500e+06</td>\n",
       "      <td>-3.975500e+06</td>\n",
       "      <td>-3.967500e+06</td>\n",
       "      <td>-3.959500e+06</td>\n",
       "      <td>-3.951500e+06</td>\n",
       "      <td>-3.943500e+06</td>\n",
       "      <td>-3.935500e+06</td>\n",
       "      <td>-3.927500e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.920500e+06</td>\n",
       "      <td>3.928500e+06</td>\n",
       "      <td>3.936500e+06</td>\n",
       "      <td>3.944500e+06</td>\n",
       "      <td>3.952500e+06</td>\n",
       "      <td>3.960501e+06</td>\n",
       "      <td>3.968500e+06</td>\n",
       "      <td>3.976500e+06</td>\n",
       "      <td>3.984500e+06</td>\n",
       "      <td>3.992500e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-03 12:00:00</th>\n",
       "      <td>-3.999500e+06</td>\n",
       "      <td>-3.991500e+06</td>\n",
       "      <td>-3.983500e+06</td>\n",
       "      <td>-3.975500e+06</td>\n",
       "      <td>-3.967500e+06</td>\n",
       "      <td>-3.959500e+06</td>\n",
       "      <td>-3.951500e+06</td>\n",
       "      <td>-3.943500e+06</td>\n",
       "      <td>-3.935500e+06</td>\n",
       "      <td>-3.927500e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.920500e+06</td>\n",
       "      <td>3.928500e+06</td>\n",
       "      <td>3.936500e+06</td>\n",
       "      <td>3.944500e+06</td>\n",
       "      <td>3.952500e+06</td>\n",
       "      <td>3.960501e+06</td>\n",
       "      <td>3.968500e+06</td>\n",
       "      <td>3.976500e+06</td>\n",
       "      <td>3.984500e+06</td>\n",
       "      <td>3.992500e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-04 12:00:00</th>\n",
       "      <td>-3.999500e+06</td>\n",
       "      <td>-3.991500e+06</td>\n",
       "      <td>-3.983500e+06</td>\n",
       "      <td>-3.975500e+06</td>\n",
       "      <td>-3.967500e+06</td>\n",
       "      <td>-3.959500e+06</td>\n",
       "      <td>-3.951500e+06</td>\n",
       "      <td>-3.943500e+06</td>\n",
       "      <td>-3.935500e+06</td>\n",
       "      <td>-3.927500e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.920500e+06</td>\n",
       "      <td>3.928500e+06</td>\n",
       "      <td>3.936500e+06</td>\n",
       "      <td>3.944500e+06</td>\n",
       "      <td>3.952500e+06</td>\n",
       "      <td>3.960501e+06</td>\n",
       "      <td>3.968500e+06</td>\n",
       "      <td>3.976500e+06</td>\n",
       "      <td>3.984500e+06</td>\n",
       "      <td>3.992500e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 1000000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0             1             2             3       \\\n",
       "2017-04-01 12:00:00 -3.999500e+06 -3.991500e+06 -3.983500e+06 -3.975500e+06   \n",
       "2017-04-02 12:00:00 -3.999500e+06 -3.991500e+06 -3.983500e+06 -3.975500e+06   \n",
       "2017-04-03 12:00:00 -3.999500e+06 -3.991500e+06 -3.983500e+06 -3.975500e+06   \n",
       "2017-04-04 12:00:00 -3.999500e+06 -3.991500e+06 -3.983500e+06 -3.975500e+06   \n",
       "\n",
       "                           4             5             6             7       \\\n",
       "2017-04-01 12:00:00 -3.967500e+06 -3.959500e+06 -3.951500e+06 -3.943500e+06   \n",
       "2017-04-02 12:00:00 -3.967500e+06 -3.959500e+06 -3.951500e+06 -3.943500e+06   \n",
       "2017-04-03 12:00:00 -3.967500e+06 -3.959500e+06 -3.951500e+06 -3.943500e+06   \n",
       "2017-04-04 12:00:00 -3.967500e+06 -3.959500e+06 -3.951500e+06 -3.943500e+06   \n",
       "\n",
       "                           8             9       ...        999990  \\\n",
       "2017-04-01 12:00:00 -3.935500e+06 -3.927500e+06  ...  3.920500e+06   \n",
       "2017-04-02 12:00:00 -3.935500e+06 -3.927500e+06  ...  3.920500e+06   \n",
       "2017-04-03 12:00:00 -3.935500e+06 -3.927500e+06  ...  3.920500e+06   \n",
       "2017-04-04 12:00:00 -3.935500e+06 -3.927500e+06  ...  3.920500e+06   \n",
       "\n",
       "                           999991        999992        999993        999994  \\\n",
       "2017-04-01 12:00:00  3.928500e+06  3.936500e+06  3.944500e+06  3.952500e+06   \n",
       "2017-04-02 12:00:00  3.928500e+06  3.936500e+06  3.944500e+06  3.952500e+06   \n",
       "2017-04-03 12:00:00  3.928500e+06  3.936500e+06  3.944500e+06  3.952500e+06   \n",
       "2017-04-04 12:00:00  3.928500e+06  3.936500e+06  3.944500e+06  3.952500e+06   \n",
       "\n",
       "                           999995        999996        999997        999998  \\\n",
       "2017-04-01 12:00:00  3.960501e+06  3.968500e+06  3.976500e+06  3.984500e+06   \n",
       "2017-04-02 12:00:00  3.960501e+06  3.968500e+06  3.976500e+06  3.984500e+06   \n",
       "2017-04-03 12:00:00  3.960501e+06  3.968500e+06  3.976500e+06  3.984500e+06   \n",
       "2017-04-04 12:00:00  3.960501e+06  3.968500e+06  3.976500e+06  3.984500e+06   \n",
       "\n",
       "                           999999  \n",
       "2017-04-01 12:00:00  3.992500e+06  \n",
       "2017-04-02 12:00:00  3.992500e+06  \n",
       "2017-04-03 12:00:00  3.992500e+06  \n",
       "2017-04-04 12:00:00  3.992500e+06  \n",
       "\n",
       "[4 rows x 1000000 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
